{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatbot For Medical Advisory using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK stands for Natural Language ToolKit. It has many algorithms available for text tokenisation, stemming, classification and clustering etc. It is also helpful Python programs which work with human language data for applying in statistical natural language processing.\n",
    "WordNetLemmatizer contains two words - Wordnet (It is freely available lexical database for the English language which aim to establish structural semantic relationships between words.) and Lemmatizing (It is a process of grouping together the different inflected forms of a word, so that they can be analysed together.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\as041\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\as041\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is an open source software library that provides a python interface for Artificial Neural Networks. It runs on top of tensorflow. It helps in building Machine Learning Models. Keras.models helps in importing required model for your project. Keras.layers help in adding hidden layers to our model. Keras.optimisers helps in importing correct optimiztion criteria for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is a python library, helps in performing various mathematical functions on arrays.\n",
    "JSON is a standard file format and data interchange format that uses human readable text to store and interchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intents.JSON is the Project Data File. It contains list of various intents['tags', 'pattern', 'responses', 'context'] in which intents['tags] defines about the criteria in which a user question falls, intents[pattern'] defines questions (same questions can be asked in many ways) and intents['pattern'] defines about the reponses the user will get on asking the question present in intents['pattern]. Intents['context] tells about relation of one intent to other intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intents Loaded as Dataset\n"
     ]
    }
   ],
   "source": [
    "# Loading intents in the workbook as a training dataset\n",
    "intents = {}\n",
    "with open(\"Chatbot_intents.json\") as f:\n",
    "    intents = json.load(f)\n",
    "print(\"Intents Loaded as Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words is an array which conatain total number of unique words used in the formation of intents['patterns'] in Intents.JSON.\n",
    "Classes is an array which contain the intents['tags'] under which the user question patterns criteria is formed.\n",
    "Documents is an array which contain intents['pattern'] with intents['tags'], whcih is helpful in generating input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    # Creating a list of intents['tags'] to define various criteria under which user questions can fall\n",
    "    classes.append(intent['tag'])\n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenizing every question pattern present in intent['patterns'] in intents['intents] of intents.JSON\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # Extending every tokezised word in Words to train the model\n",
    "        words.extend(w)\n",
    "        # Documents are appended with tokenized words with their intents['tags'] to create a corpus of data for model \n",
    "        documents.append((w, intent['tag']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words array contain every tokenized words in the intents['pattern'] of intents.JSON, which account for\n",
    "# many duplicate words and unwanted words in the Words[]. Also, every word in the Words[] array should be\n",
    "# in lowercase alphabetical form for better understanding and comparison.\n",
    "# WordNetLemmatizer is used to remove duplicacy and unwanted words from the Words[].\n",
    "ignore_words = ['?', '!', ',']\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328 Documents\n",
      "71 Classes\n",
      "146 Unique Lemmatized Words\n"
     ]
    }
   ],
   "source": [
    "# Documents[] tells about the total number of combination between the intents['tags'] and intents['pattern'] has been formed\n",
    "print(len(documents), \"Documents\")\n",
    "\n",
    "# Sorting the classes in Alphabetical Order\n",
    "classes = sorted(list(set(classes)))\n",
    "# Length of Classes[] will tell about the number of intents our chatbot would be using\n",
    "# while comparing or responding back to the user\n",
    "print(len(classes), \"Classes\")\n",
    "\n",
    "# set() will find out unique words from the Words[] and sorted() will sort them into alphabetical order\n",
    "words = sorted(list(set(words)))\n",
    "# Words[] conatins unique words used in the formation of the the pattern of questions  \n",
    "print(len(words), \"Unique Lemmatized Words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"words.json\", \"w\") as f:\n",
    "#     json.dump(words, f)\n",
    "#     print(\"Words Done\")\n",
    "    \n",
    "# with open(\"classes.json\", \"w\") as f:\n",
    "#     json.dump(classes, f)\n",
    "#     print(\"Classes Done\")\n",
    "\n",
    "# with open(\"documents.json\", \"w\") as f:\n",
    "#     json.dump(documents, f)\n",
    "#     print(\"Dosuments Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Created\n"
     ]
    }
   ],
   "source": [
    "# Creating training array to store training data\n",
    "training = []\n",
    "\n",
    "# Creating training corpus for the model\n",
    "for doc in documents:\n",
    "    # Taking out tokenised word list stored in documents at 0th position\n",
    "    pattern_words = doc[0]\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    \n",
    "    # Defining an array to store the words in 0 & 1 as 1 represents words present in Unique Lemmatize Words[]\n",
    "    bag = []\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "    # Creating an array \"output_row\" having length equal to Classes[]   \n",
    "    output_row = list([0] * len(classes))\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# Random shuffle the training list and convert it into a numpy array\n",
    "random.shuffle(training)\n",
    "training = numpy.array(training, dtype=object)\n",
    "\n",
    "# Creating training and testing corpus in which X - contains patterns from intents.JSON & Y - contains intents from intents.JSON\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])\n",
    "\n",
    "print(\"Training Data Created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Model For Chatbot and training the model with different intent and hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model, we choose tensorflow.keras.models.Sequential(). We added 3 tensorflow.keras.layers.Dense() layers and 2 tensorflow.keras.layers.Dropout() layers to create the model. First Dense layers named \"layer_1\" contain 128 neurons with input shape equal to length(train_x[0]) (or length(words[])), having activation function \"relu\". Second Dense layer named \"layer_2\" contain 64 neurons with input equal to the output of layer_1, having activation function \"relu\". Third Dense layer is named \"layer_3\" whose number of neurons equal to len(train_y[0]) (or len(classes[])) with activation function as \"softmax\". Two Dropouts layers name \"dropout_1\" and dropout_2\" are added to reduce overfitting and overall loss of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model compilation, tensorflow.keras.optimizers.RMSprop is used which gives best accuracy with minimal \"categorical_crossentropy\" loss. After compilation, training of model is done. For training in model.fit, x - train_x, y - train_y is used, with batch_size of 5, total number of epochs used are 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_1 (Dense)             (None, 256)               37632     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " layer_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " layer_3 (Dense)             (None, 71)                9159      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,687\n",
      "Trainable params: 79,687\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "17/17 [==============================] - 1s 3ms/step - loss: 4.2583 - accuracy: 0.0183\n",
      "Epoch 2/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 4.2169 - accuracy: 0.0396\n",
      "Epoch 3/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 4.1494 - accuracy: 0.0640\n",
      "Epoch 4/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 4.0867 - accuracy: 0.0518\n",
      "Epoch 5/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 4.0463 - accuracy: 0.0457\n",
      "Epoch 6/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.9504 - accuracy: 0.0884\n",
      "Epoch 7/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.8474 - accuracy: 0.0976\n",
      "Epoch 8/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.7586 - accuracy: 0.0976\n",
      "Epoch 9/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.6365 - accuracy: 0.1524\n",
      "Epoch 10/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.5298 - accuracy: 0.1616\n",
      "Epoch 11/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.3864 - accuracy: 0.2195\n",
      "Epoch 12/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.2025 - accuracy: 0.2744\n",
      "Epoch 13/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.1352 - accuracy: 0.2561\n",
      "Epoch 14/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.9404 - accuracy: 0.3750\n",
      "Epoch 15/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.7842 - accuracy: 0.3659\n",
      "Epoch 16/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.6678 - accuracy: 0.3872\n",
      "Epoch 17/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.4734 - accuracy: 0.4695\n",
      "Epoch 18/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.3201 - accuracy: 0.4695\n",
      "Epoch 19/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2.2240 - accuracy: 0.5030\n",
      "Epoch 20/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2.0020 - accuracy: 0.5213\n",
      "Epoch 21/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.9107 - accuracy: 0.5640\n",
      "Epoch 22/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.8355 - accuracy: 0.6067\n",
      "Epoch 23/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.7291 - accuracy: 0.5823\n",
      "Epoch 24/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.5051 - accuracy: 0.6646\n",
      "Epoch 25/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.4742 - accuracy: 0.6372\n",
      "Epoch 26/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.3360 - accuracy: 0.6951\n",
      "Epoch 27/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.2139 - accuracy: 0.7317\n",
      "Epoch 28/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.2327 - accuracy: 0.7012\n",
      "Epoch 29/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.0310 - accuracy: 0.7561\n",
      "Epoch 30/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.0423 - accuracy: 0.7500\n",
      "Epoch 31/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.9876 - accuracy: 0.7591\n",
      "Epoch 32/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.9464 - accuracy: 0.7591\n",
      "Epoch 33/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.8298 - accuracy: 0.8262\n",
      "Epoch 34/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.7587 - accuracy: 0.8323\n",
      "Epoch 35/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.7825 - accuracy: 0.8018\n",
      "Epoch 36/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6444 - accuracy: 0.8628\n",
      "Epoch 37/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6715 - accuracy: 0.8232\n",
      "Epoch 38/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6706 - accuracy: 0.8384\n",
      "Epoch 39/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6308 - accuracy: 0.8354\n",
      "Epoch 40/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5857 - accuracy: 0.8628\n",
      "Epoch 41/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5901 - accuracy: 0.8598\n",
      "Epoch 42/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4922 - accuracy: 0.9085\n",
      "Epoch 43/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5009 - accuracy: 0.8780\n",
      "Epoch 44/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4145 - accuracy: 0.9146\n",
      "Epoch 45/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4484 - accuracy: 0.9085\n",
      "Epoch 46/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.3857 - accuracy: 0.9238\n",
      "Epoch 47/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.3790 - accuracy: 0.8994\n",
      "Epoch 48/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.3594 - accuracy: 0.9146\n",
      "Epoch 49/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.3424 - accuracy: 0.9207\n",
      "Epoch 50/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.3117 - accuracy: 0.9421\n",
      "Epoch 51/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.3010 - accuracy: 0.9268\n",
      "Epoch 52/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.2842 - accuracy: 0.9451\n",
      "Epoch 53/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.2746 - accuracy: 0.9360\n",
      "Epoch 54/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.2598 - accuracy: 0.9390\n",
      "Epoch 55/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.2722 - accuracy: 0.9390\n",
      "Epoch 56/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.2880 - accuracy: 0.9329\n",
      "Epoch 57/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.2281 - accuracy: 0.9573\n",
      "Epoch 58/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.2324 - accuracy: 0.9299\n",
      "Epoch 59/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.1919 - accuracy: 0.9726\n",
      "Epoch 60/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.2050 - accuracy: 0.9604\n",
      "Epoch 61/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1807 - accuracy: 0.9634\n",
      "Epoch 62/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.2573 - accuracy: 0.9238\n",
      "Epoch 63/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1932 - accuracy: 0.9451\n",
      "Epoch 64/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1801 - accuracy: 0.9543\n",
      "Epoch 65/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1503 - accuracy: 0.9756\n",
      "Epoch 66/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1881 - accuracy: 0.9604\n",
      "Epoch 67/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1713 - accuracy: 0.9512\n",
      "Epoch 68/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1915 - accuracy: 0.9543\n",
      "Epoch 69/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1370 - accuracy: 0.9634\n",
      "Epoch 70/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1482 - accuracy: 0.9665\n",
      "Epoch 71/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1344 - accuracy: 0.9726\n",
      "Epoch 72/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1425 - accuracy: 0.9665\n",
      "Epoch 73/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1128 - accuracy: 0.9848\n",
      "Epoch 74/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1249 - accuracy: 0.9848\n",
      "Epoch 75/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1379 - accuracy: 0.9634\n",
      "Epoch 76/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1165 - accuracy: 0.9787\n",
      "Epoch 77/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1222 - accuracy: 0.9695\n",
      "Epoch 78/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1147 - accuracy: 0.9756\n",
      "Epoch 79/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1279 - accuracy: 0.9634\n",
      "Epoch 80/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1141 - accuracy: 0.9726\n",
      "Epoch 81/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1207 - accuracy: 0.9848\n",
      "Epoch 82/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.9817\n",
      "Epoch 83/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0980 - accuracy: 0.9756\n",
      "Epoch 84/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.1040 - accuracy: 0.9756\n",
      "Epoch 85/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0774 - accuracy: 0.9848\n",
      "Epoch 86/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0724 - accuracy: 0.9848\n",
      "Epoch 87/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0904 - accuracy: 0.9787\n",
      "Epoch 88/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0558 - accuracy: 0.9909\n",
      "Epoch 89/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0792 - accuracy: 0.9817\n",
      "Epoch 90/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0819 - accuracy: 0.9756\n",
      "Epoch 91/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0882 - accuracy: 0.9726\n",
      "Epoch 92/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0811 - accuracy: 0.9756\n",
      "Epoch 93/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0943 - accuracy: 0.9726\n",
      "Epoch 94/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0923 - accuracy: 0.9756\n",
      "Epoch 95/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0762 - accuracy: 0.9787\n",
      "Epoch 96/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.9970\n",
      "Epoch 97/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9848\n",
      "Epoch 98/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9817\n",
      "Epoch 99/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9848\n",
      "Epoch 100/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.9756\n",
      "Epoch 101/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9909\n",
      "Epoch 102/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9848\n",
      "Epoch 103/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0462 - accuracy: 0.9939\n",
      "Epoch 104/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1029 - accuracy: 0.9695\n",
      "Epoch 105/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9878\n",
      "Epoch 106/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9878\n",
      "Epoch 107/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0549 - accuracy: 0.9878\n",
      "Epoch 108/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0439 - accuracy: 0.9909\n",
      "Epoch 109/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9909\n",
      "Epoch 110/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9848\n",
      "Epoch 111/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0425 - accuracy: 0.9939\n",
      "Epoch 112/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9848\n",
      "Epoch 113/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.9726\n",
      "Epoch 114/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9817\n",
      "Epoch 115/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9878\n",
      "Epoch 116/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.9970\n",
      "Epoch 117/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0536 - accuracy: 0.9817\n",
      "Epoch 118/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0795 - accuracy: 0.9695\n",
      "Epoch 119/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0496 - accuracy: 0.9787\n",
      "Epoch 120/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 0.9970\n",
      "Epoch 121/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0380 - accuracy: 0.9909\n",
      "Epoch 122/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.9970\n",
      "Epoch 123/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9787\n",
      "Epoch 124/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.9939\n",
      "Epoch 125/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9878\n",
      "Epoch 126/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0364 - accuracy: 0.9878\n",
      "Epoch 127/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0382 - accuracy: 0.9878\n",
      "Epoch 128/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.9939\n",
      "Epoch 129/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9848\n",
      "Epoch 130/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0422 - accuracy: 0.9878\n",
      "Epoch 131/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9787\n",
      "Epoch 132/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0340 - accuracy: 0.9878\n",
      "Epoch 133/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.9909\n",
      "Epoch 135/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0299 - accuracy: 0.9909\n",
      "Epoch 136/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9817\n",
      "Epoch 137/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.9939\n",
      "Epoch 138/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.9909\n",
      "Epoch 139/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0333 - accuracy: 0.9909\n",
      "Epoch 140/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0303 - accuracy: 0.9909\n",
      "Epoch 141/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.9909\n",
      "Epoch 142/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.9909\n",
      "Epoch 144/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0278 - accuracy: 0.9939\n",
      "Epoch 145/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0296 - accuracy: 0.9939\n",
      "Epoch 146/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 0.9970\n",
      "Epoch 147/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.9970\n",
      "Epoch 148/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 0.9848\n",
      "Epoch 149/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0354 - accuracy: 0.9878\n",
      "Epoch 150/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.9939\n",
      "Epoch 151/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 0.9970\n",
      "Epoch 152/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0293 - accuracy: 0.9939\n",
      "Epoch 153/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0329 - accuracy: 0.9878\n",
      "Epoch 154/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0386 - accuracy: 0.9909\n",
      "Epoch 155/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.9939\n",
      "Epoch 156/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.9939\n",
      "Epoch 157/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.9970\n",
      "Epoch 158/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 0.9970\n",
      "Epoch 159/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 0.9878\n",
      "Epoch 160/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.9909\n",
      "Epoch 161/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.9970\n",
      "Epoch 163/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0166 - accuracy: 0.9939\n",
      "Epoch 164/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 0.9970\n",
      "Epoch 165/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.9909\n",
      "Epoch 166/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.9939\n",
      "Epoch 167/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0232 - accuracy: 0.9909\n",
      "Epoch 168/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 0.9939\n",
      "Epoch 169/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0321 - accuracy: 0.9848\n",
      "Epoch 170/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0214 - accuracy: 0.9939\n",
      "Epoch 171/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9970\n",
      "Epoch 172/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.9939\n",
      "Epoch 173/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.9878\n",
      "Epoch 174/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.9939\n",
      "Epoch 175/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 0.9970\n",
      "Epoch 176/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 0.9909\n",
      "Epoch 178/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0125 - accuracy: 0.9970\n",
      "Epoch 179/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.9970\n",
      "Epoch 180/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0205 - accuracy: 0.9909\n",
      "Epoch 181/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0214 - accuracy: 0.9939\n",
      "Epoch 182/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.9970\n",
      "Epoch 183/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.9970\n",
      "Epoch 184/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0167 - accuracy: 0.9939\n",
      "Epoch 185/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.9878\n",
      "Epoch 186/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.9878\n",
      "Epoch 189/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.9970\n",
      "Epoch 190/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.9970\n",
      "Epoch 191/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.9970\n",
      "Epoch 193/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9939\n",
      "Epoch 196/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.9939\n",
      "Epoch 197/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 0.9970\n",
      "Epoch 200/200\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 0.9939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e377d46370>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the model\n",
    "model = Sequential()\n",
    "# Adding first Dense layer\n",
    "model.add(Dense(256, input_shape=(len(train_x[0]),), activation='relu', name = \"layer_1\"))\n",
    "# Adding first Dropout layer\n",
    "model.add(Dropout(0.5, name = \"dropout_1\"))\n",
    "# Adding second Dense layer\n",
    "model.add(Dense(128, activation='relu', name = \"layer_2\"))\n",
    "# Adding second Dropout layer\n",
    "model.add(Dropout(0.5, name = \"dropout_2\"))\n",
    "# Adding third Dense layer\n",
    "model.add(Dense(len(train_y[0]), activation='softmax', name = \"layer_3\"))\n",
    "\n",
    "# Tells about the summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSProp', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(numpy.array(train_x), numpy.array(train_y),\n",
    "                  epochs=200, batch_size=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell tells about the \"Total Model Loss\" and \"Total Model Accuracy\". Using \"RMSprop\" optimizers, which gives the least loss and highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 1.0608e-04 - accuracy: 1.0000\n",
      "Model Loss:  0.00010608316370053217\n",
      "Model Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# This cell tells about the evaluation metrics of our model\n",
    "score = model.evaluate(numpy.array(train_x), numpy.array(train_y), verbose=True)\n",
    "\n",
    "# It prints about the Model Loss\n",
    "print(\"Model Loss: \" , score[0])\n",
    "\n",
    "# It prints about the Model Accuracy\n",
    "print(\"Model Accuracy: \" , score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Program for getting responses from the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps in creating a bag[] which can be used as a test data for our model\n",
    "# \"sentence\" arguments contain the question pattern asked by the user.\n",
    "# \"words\" refers to the words[] which contain unique lemmatize words from intents.JSON  \n",
    "def sentence_in_words(sentence):\n",
    "    # \"sentence\" gets tokenize and lowered in order to match the holdings of words[]\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in nltk.word_tokenize(sentence)]\n",
    "    \n",
    "    # Creating a bag[] whose length equal to words[] in order to compare it with the train_x[]\n",
    "    bag = [0] * len(words)\n",
    "    \n",
    "    # Substituting 1 at places where \"sentence_words\" is found in the words[] in order to\n",
    "    # generate input of the model\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "    \n",
    "    return (numpy.array(bag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps in predicting the intent i.e. the intents['tag'] under which question\n",
    "# has highest falling probability and return the intent name with probability\n",
    "# \"sentence\" argument contains the question pattern asked by the user\n",
    "# \"model\" refers to the model created for the project\n",
    "def predicting_intent(sentence):\n",
    "    # bag[] containing the compared data from the words[] obtained from the sentence_in_words () \n",
    "    bag = sentence_in_words(sentence)\n",
    "    \n",
    "    # predicting the intents['tag'] under which the bag[] may fall\n",
    "    res = model.predict(numpy.array([bag]))[0]\n",
    "    \n",
    "    # [i, r] where i contain the index of classes[], of which intent['tags'] user question belongs\n",
    "    # and r conatins the predicted probability of user pattern falling into the intents \n",
    "    # It is possible to have more than one intent with same pattern, then the intent with highest probability will be considered\n",
    "    # results.sort() sorts the result in the descending order on the basis of the probability predicted\n",
    "    results = [[i, r] for i, r in enumerate(res)]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # finding the intents['tags'] with the help of classes[] of the result[0] which has the \n",
    "    # highest probability (from whcih the user question pattern may belong), if probability is not found, then\n",
    "    # \"do not understand question\" reaponse get geenrated\n",
    "    if (results[0][0] < 0.50):\n",
    "        results = [{\"intent\": 'i do not understand', \"probability\": str(results[0][1])}]\n",
    "    else:\n",
    "        results = [{\"intent\": classes[results[0][0]], \"probability\": str(results[0][1])}]\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates the actual response to be given by the chatbot, for the user's question pattern\n",
    "def get_Response(ints):\n",
    "    # ints contain the result from the predicting_intent() which has both intent name and its probability\n",
    "    # tag variable gets the intent name from the ints\n",
    "    tag = ints[0]['intent']\n",
    "    \n",
    "    # comparing the tag name with the intents['tags'] of intents.JSON to generate a random response\n",
    "    for i in intents['intents']:\n",
    "        if(i['tag'] == tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function to call other function\n",
    "def chatbot_response(msg):\n",
    "    # This function returns with the intent['tags'] with highest probability \n",
    "    # under which the \"msg\" has fallen\n",
    "    ints = predicting_intent(msg)\n",
    "    \n",
    "    # This function returns with one of the random responses from the intents['responses']\n",
    "    # related to the user question pattern\n",
    "    response = get_Response(ints)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, How can i help?\n"
     ]
    }
   ],
   "source": [
    "# Ask Your Ques Here\n",
    "ques = \"Hello\"\n",
    "print(chatbot_response(ques))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7f6e885f8e622edb2461906a732942c996165817b40ef9b5ba659c5716f37ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
